# -*- coding: utf-8 -*-
"""Deep_Networks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10tqIDTxmCctB_UHUIH112F-mc7Ltlvxh
"""

# Commented out IPython magic to ensure Python compatibility.
# Basic imports.
import os
import time
import random
import numpy as np
import argparse
import torch
import torchvision
import copy

from torch import nn
from torch import optim
from torch.nn import functional as F

from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torch.utils import data
from torch.backends import cudnn

from torchvision import models
from torchvision import datasets
from torchvision import transforms

from skimage import io
from skimage import transform as T

from sklearn import metrics

from matplotlib import pyplot as plt

from net import Segnet, SegnetMT
from blocks import ConvReLU, EncoderBlock, DecoderBlockS, DecoderBlockU
from datasets import CoffeeDataset, CoffeeDatasetModular, CoffeeDatasetModularMT

import warnings
warnings.filterwarnings("ignore")

from train_test import trainMT, testMT

eps = 10e-5

def train(train_loader, net, criterion, optimizer, epoch, freeze_enc=False, segmentation=False, evaluation=None, args=None):

    tic = time.time()
    
    # Setting network for training mode.
    net.train()
    if freeze_enc and segmentation:
        net.encoder.eval()

    # Average Meter for batch loss.
    train_loss = []

    # Lists for whole epoch loss.
    labs_all, prds_all = [], []

    # Iterating over batches.
    for i, batch_data in enumerate(train_loader):

        # Obtaining images and labels for batch.
        inps, labs = batch_data

        # Casting to cuda variables.
        inps = torch.Tensor(inps).to(args['device'])
        #labs = torch.Tensor(labs).to(args['device']).to(torch.int64).squeeze()
        labs = torch.as_tensor(labs, device=args['device']).to(torch.int64)
        
        # Clears the gradients of optimizer.
        optimizer.zero_grad()

        # Forwarding.
        outs = net(inps)

        # Computing loss.
        if segmentation:
            loss = criterion(outs, labs)
        else:
            loss = criterion(outs, inps)

        # Obtaining predictions.
        if segmentation:
            prds = outs.data.max(1)[1].squeeze(1).cpu().numpy()
        else:
            prds = outs.data.cpu().numpy()

        # Appending images for epoch loss calculation.
        labs_all.append(labs.detach().data.squeeze(1).cpu().numpy())
        prds_all.append(prds)

        # Computing backpropagation.
        loss.backward()
        optimizer.step()

        # Updating loss meter.
        train_loss.append(loss.data.item())

    toc = time.time()
    
    # Transforming list into numpy array.
    train_loss = np.asarray(train_loss)
    
    # Printing training epoch loss and metrics.
    # Computing error metrics for whole epoch.

    iou, normalized_acc = np.zeros(1), np.zeros(1)
    if epoch % args['pf'] == 0:
        if not (evaluation is None):
            iou, normalized_acc = evaluation(prds_all, labs_all) #evaluate(prds_all, labs_all, args['n_classes'])
            # Printing test epoch loss and metrics.
            print('-------------------------------------------------------------------')
            print('[epoch %d], [train loss %.4f +/- %.4f], [iou %.4f +/- %.4f], [normalized acc %.4f +/- %.4f], [time %.2f]' % (
                epoch, train_loss.mean(), train_loss.std(), iou.mean(), iou.std(), normalized_acc.mean(), normalized_acc.std(), (toc - tic)))
            print('-------------------------------------------------------------------')

        else:
            print('-------------------------------------------------------------------')
            print('[epoch %d], [train loss %.4f +/- %.4f], [time %.2f]' % (
                epoch, train_loss.mean(), train_loss.std(), (toc - tic)))
            print('-------------------------------------------------------------------')

    if not (evaluation is None):
        return 1.0-normalized_acc.mean(), normalized_acc.std()
    return train_loss.mean(), train_loss.std()


def test(test_loader, net, criterion, epoch, segmentation=False, evaluation=None, outputfolder=None, args=None):

    tic = time.time()
    
    # Setting network for evaluation mode.
    net.eval()

    # Average Meter for batch loss.
    test_loss = []

    # Lists for whole epoch loss.
    labs_all, prds_all = [], []

    # Iterating over batches.
    for i, batch_data in enumerate(test_loader):
        # Obtaining images and labels for batch.
        inps, labs = batch_data

        # Casting to cuda variables.
        inps = torch.Tensor(inps).to(args['device'])
        #labs = torch.Tensor(labs).to(args['device']).to(torch.int64).squeeze()
        labs = torch.as_tensor(labs, device=args['device']).to(torch.int64)
        
        # Forwarding.
        outs = net(inps)
        #print(outs.shape)
        
        # Computing loss.
        if segmentation:
            loss = criterion(outs, labs)
        else:
            loss = criterion(outs, inps)

        # Obtaining predictions.
        if segmentation:
            prds = outs.data.max(1)[1].squeeze(1).cpu().numpy()
        else:
            prds = outs.data.cpu().numpy()

        # Appending images for epoch loss calculation.
        labs_all.append(labs.detach().data.squeeze(1).cpu().numpy())
        prds_all.append(prds)

        # Updating loss meter.
        test_loss.append(loss.data.item())
    
    toc = time.time()
    
    # Transforming list into numpy array.
    test_loss = np.asarray(test_loss)
    
    # Computing error metrics for whole epoch.
    iou, normalized_acc = np.zeros(1), np.zeros(1)
    if epoch % args['pf'] == 0:
        if not (evaluation is None):
            iou, normalized_acc = evaluation(prds_all, labs_all) #evaluate(prds_all, labs_all, args['n_classes'])
            # Printing test epoch loss and metrics.
            print('-------------------------------------------------------------------')
            print('[epoch %d], [test loss %.4f +/- %.4f], [iou %.4f +/- %.4f], [normalized acc %.4f +/- %.4f], [time %.2f]' % (
                epoch, test_loss.mean(), test_loss.std(), iou.mean(), iou.std(), normalized_acc.mean(), normalized_acc.std(), (toc - tic)))
            print('-------------------------------------------------------------------')

        else:
            print('-------------------------------------------------------------------')
            print('[epoch %d], [test loss %.4f +/- %.4f], [time %.2f]' % (
                epoch, test_loss.mean(), test_loss.std(), (toc - tic)))
            print('-------------------------------------------------------------------')


    if not outputfolder is None:
        if not os.path.exists(outputfolder):
            os.mkdir(outputfolder)
        #print(len(prds_all))
        for i, img in enumerate(prds_all[0]):
            #print("SHAPE", img.shape)
            name = ""
            if segmentation:
                name = 'seg_'
                name += "pred_" + str(i) + '_epoch_' + str(epoch) + '.png'

                io.imsave(os.path.join(outputfolder,name), img.astype('uint8')*255)
            else:
                name = 'rec_'
                name += "pred_" + str(i) + '_epoch_' + str(epoch) + '.png'
                sv = (np.abs(np.min(img)) + np.moveaxis(img, 0, -1))/np.max(img + np.abs(np.min(img)))
                io.imsave(os.path.join(outputfolder,name), sv)

    #if not (evaluation is None):
    #    return 1.0-normalized_acc.mean(), normalized_acc.std()
    #return test_loss.mean(), test_loss.std()
    return test_loss.mean(), normalized_acc.mean()

def evaluation(pred, lab):
    iou = []
    nacc = []
    for p, l in zip(pred, lab):
        iou.append(metrics.jaccard_similarity_score(l.flatten(), p.flatten()))
        nacc.append(metrics.balanced_accuracy_score(l.flatten(), p.flatten()))

    return np.array(iou), np.array(nacc)


def trainval(datatrain, dataval, net, criterion, optimizer, epochs, init_epoch=0, evaluation=None, outputfolder=None, freeze_enc=False, segmentation=False, patience=20, multitask=False, args=None):
    min_test_stop_criteria = float('inf')
    max_test_nacc = -float('inf')
    count = 0
    best_epoch = 0
    best_nacc_epoch = 0
    net_wt = None
    for epoch in range(init_epoch, epochs):

        if multitask:
            # Training function.
            train_stop_criteria, _ = trainMT(datatrain, net, criterion[0], criterion[1],
                                              optimizer, epoch, args=args)

            # Computing test loss and metrics.
            test_stop_criteria, nacc = testMT(dataval, net, criterion[0], criterion[1], epoch, 
                                            evaluation=evaluation, segmentation=segmentation, 
                                            outputfolder=outputfolder, args=args)

        else:
            # Training function.
            train_stop_criteria, _ = train(datatrain, net, criterion, 
                                              optimizer, epoch, freeze_enc=freeze_enc, segmentation=segmentation, args=args)

            # Computing test loss and metrics.
            test_stop_criteria, nacc = test(dataval, net, criterion, epoch, 
                                            evaluation=evaluation, segmentation=segmentation, 
                                            outputfolder=outputfolder, args=args)


        if nacc > max_test_nacc:
            max_test_nacc = nacc
            best_nacc_epoch = epoch

        # Early Stop Checking
        if test_stop_criteria < min_test_stop_criteria:
            min_test_stop_criteria = test_stop_criteria
            best_epoch = epoch
            count = 0

            net_wt = copy.deepcopy(net.state_dict())

        elif count >= patience:
            print("-----------------------------------------")
            print("Early Stopping the training. The criteria didn't reach a new minimum in %d epochs." % patience)
            print("-----------------------------------------")
            break
        count += 1


    print("-----------------------------------------")
    print("Best test Normalized Acc %.4f at epoch %d" %(max_test_nacc, best_nacc_epoch))
    print("Best result is: %.4f at epoch %d" % (min_test_stop_criteria, best_epoch))
    print("-----------------------------------------")

    torch.save(net_wt, os.path.join(args['model_save_path'], args['model_save_name']))



def main():
    cudnn.benchmark = True

    # Setting predefined arguments.
    parser = argparse.ArgumentParser()
    parser.add_argument('-e', '--epochs', help='number of training epochs', required=True, type=int)
    parser.add_argument('--ftepochs', '--fte', help='number of training epochs', type=int)
    parser.add_argument('--lr', help='learning rate', default=5e-4, type=float)
    parser.add_argument('--wd', help='weight decay(l2 penalty)', default=5e-5, type=float)
    parser.add_argument('-u','--usablelabels', help='number of training epochs', type=float)
    parser.add_argument('-b', '--batch_size', help='number of samples in a mini-batch', required=True, type=int)
    parser.add_argument('--pf', '--print_freq', help='print frequency of loss values', default=1, type=int)
    parser.add_argument('--inputsize', help='number of channels in the input', default=3, type=int)
    parser.add_argument('--numblocks', help='number of encoder/decoder blocks', default=4, type=int)
    parser.add_argument('--nclasses', help='number of semantic classes', default=2, type=int)
    parser.add_argument('-p','--patience', help='patience of early stop(max number of epochs without a new minimum criteria)', default=10, type=int)


    parser.add_argument('-f', '--freeze', help='Flag to freeze or not the encoder weights', default=True, type=bool)
    parser.add_argument('--imgsfolder','--if', help='root folder to the image set', required=True)
    parser.add_argument('--maskfolder','--mf', help='root folder to the label set', required=True)
    parser.add_argument('--outputfolder', help='root folder to the outputs of the model', default='./outfolder')
    parser.add_argument('--sourceimgslist', '--sil', help='text file with a list of source images')
    parser.add_argument('--targetimgslisttrain', '--ttil', help='text file with a list of target images(train)', required=True)
    parser.add_argument('--targetimgslistval', '--tvil', help='text file with a list of target images(validation)', required=True)


    parser.add_argument('--no_transfer', '--nt', help='flag to set no transfer scenario, ie. the networks is trained on source and validated on target', default=False, type=bool)
    parser.add_argument('--multitask_net', '--mt', help='flag to use the multitask network, ie. the network that reconstruct and segment an image', default=False, type=bool)

    args = vars(parser.parse_args())

    if not os.path.exists(args['outputfolder']):
        os.mkdir(args['outputfolder'])

    # In code parameters 
    args['num_workers'] = 0#8                         # Number of workers on data loader.
    args['kernelsize'] = 3                          # Size of convolution Kernels
    args['numchannels'] = 64                        # Number of feature maps in the network
    args['model_save_path'] = os.path.join(args['outputfolder'], 'models')            # Folder path to save models
    if not os.path.exists(args['model_save_path']):
        os.mkdir(args['model_save_path'])

    args['model_save_name'] = "segnet_nb" + str(args['numblocks'])  + "_u" + str(int(args['usablelabels']*10)) + ".pt"

    if torch.cuda.is_available():
        args['device'] = torch.device('cuda')
    else:
        args['device'] = torch.device('cpu')

    #print(args['device'])
    hassource = False
    if args['sourceimgslist'] is not None:
        f = open(args['sourceimgslist'], 'r')
        r = f.read()
        args['sourceimgslist'] = eval(r)
        hassource = True

    if args['targetimgslisttrain'] is not None:
        f = open(args['targetimgslisttrain'], 'r')
        r = f.read()
        args['targetimgslisttrain'] = eval(r)

    if args['targetimgslistval'] is not None:
        f = open(args['targetimgslistval'], 'r')
        r = f.read()
        args['targetimgslistval'] = eval(r)

    if args['no_transfer']:
        args['targetimgslist'] = args['targetimgslisttrain'] + args['targetimgslistval']
        trainNoTransfer(args)
    elif args['multitask_net']:
        trainMultiTask(args)
    else:
        trainSourcePlusTarget(args, hassource)

    #print(args)


# This training procedure utilize the source labels and the available target labels in the segmentation process
# Also use all the available target imgs to pretrain the models with a reconstruction objective
def trainSourcePlusTarget(args, hassource=False):

    sourcedata = CoffeeDataset(args['imgsfolder'], args['maskfolder'], img_list=args['sourceimgslist']) if hassource else None
    targetdatarec = CoffeeDataset(args['imgsfolder'], args['maskfolder'], img_list=args['targetimgslisttrain'])
    targetdatatrain = CoffeeDatasetModular(args['imgsfolder'], args['maskfolder'], args['usablelabels'], img_list=args['targetimgslisttrain'])
    segdatatrain = CoffeeDataset(args['imgsfolder'], args['maskfolder'], img_list=sourcedata.imgs + targetdatatrain.imgs) if hassource else targetdatatrain
    
    targetdataval = CoffeeDataset(args['imgsfolder'], args['maskfolder'], img_list=args['targetimgslistval'])

    seg_loader = DataLoader(segdatatrain, batch_size=args['batch_size'], num_workers=args['num_workers'], shuffle=True)
    targetrec_loader = DataLoader(targetdatarec, batch_size=args['batch_size'], num_workers=args['num_workers'], shuffle=True)

    targetval_loader = DataLoader(targetdataval, batch_size=args['batch_size'], num_workers=args['num_workers'], shuffle=False)

    net = Segnet(inputsize=args['inputsize'], numblocks=args['numblocks'],
                 kernelsize=args['kernelsize'], nclasses=args['inputsize'],
                 numchannels=args['numchannels']).to(args['device'])

    print(net)

    optimizer = optim.Adam(net.parameters(),
                           lr=args['lr'],
                           weight_decay=args['wd'])

    # criterion = nn.CrossEntropyLoss().to(args['device'])
    criterion = nn.L1Loss().to(args['device'])
    segloss = nn.CrossEntropyLoss().to(args['device'])

    trainval(targetrec_loader, targetval_loader, net, criterion, optimizer, args['epochs'], init_epoch=0, outputfolder=args['outputfolder'], patience=args['patience'], args=args)

    # Change prediction layer
    net.decoder[-1] = DecoderBlockS(args['numchannels'], args['nclasses'], args['kernelsize'], nconvs=2).to(args['device'])

    optimizer = optim.Adam(net.parameters(),
                           lr=args['lr'],
                           weight_decay=args['wd'])
    net.to(args['device'])
    
    trainval(seg_loader, targetval_loader, net, segloss, optimizer, args['ftepochs'], init_epoch=0, evaluation=evaluation, freeze_enc=args['freeze'], segmentation=True, patience=1.5*args['patience'], outputfolder=args['outputfolder'], args=args)

# This training procedure expects a source domain
# The Network is trained in the source imgs and labels with segmentation objective but are validated in the target domain
def trainNoTransfer(args, hassource=False):
    sourcedata = CoffeeDataset(args['imgsfolder'], args['maskfolder'], img_list=args['sourceimgslist'])
    targetdata = CoffeeDataset(args['imgsfolder'], args['maskfolder'], img_list=args['targetimgslist'])

    source_loader = DataLoader(sourcedata, batch_size=args['batch_size'], num_workers=args['num_workers'], shuffle=True)
    target_loader = DataLoader(targetdata, batch_size=args['batch_size'], num_workers=args['num_workers'], shuffle=False)

    net = Segnet(inputsize=args['inputsize'], numblocks=args['numblocks'],
                 kernelsize=args['kernelsize'], nclasses=args['nclasses'],
                 numchannels=args['numchannels']).to(args['device'])

    print(net)

    optimizer = optim.Adam(net.parameters(),
                           lr=args['lr'],
                           weight_decay=args['wd'])

    segloss = nn.CrossEntropyLoss().to(args['device'])

    trainval(source_loader, target_loader, net, segloss, optimizer, args['epochs'], init_epoch=0, evaluation=evaluation, segmentation=True, patience=1.5*args['patience'], outputfolder=args['outputfolder'], args=args)


def trainMultiTask(args):
    traindata = CoffeeDatasetModularMT(args['imgsfolder'], args['maskfolder'], args['usablelabels'], img_list=args['sourceimgslist'], target_img_list=args['targetimgslisttrain'])
    valdata = CoffeeDatasetModularMT(args['imgsfolder'], args['maskfolder'], img_list=args['targetimgslistval'])

    train_loader = DataLoader(traindata, batch_size=args['batch_size'], num_workers=args['num_workers'], shuffle=True)
    val_loader = DataLoader(valdata, batch_size=args['batch_size'], num_workers=args['num_workers'], shuffle=False)

    net = SegnetMT(inputsize=args['inputsize'], numblocks=args['numblocks'],
                 kernelsize=args['kernelsize'], nclasses=args['nclasses'],
                 numchannels=args['numchannels']).to(args['device'])

    print(net)

    optimizer = optim.Adam(net.parameters(),
                           lr=args['lr'],
                           weight_decay=args['wd'])

    criterion = nn.L1Loss().to(args['device'])
    segloss = nn.CrossEntropyLoss(reduction='none').to(args['device'])


    args['rec_wt'] = 0.5 - 0.5*args['usablelabels'] + eps

    trainval(train_loader, val_loader, net, (criterion, segloss), optimizer, args['epochs'], init_epoch=0, evaluation=evaluation, multitask=True, segmentation=True, patience=1.5*args['patience'], outputfolder=args['outputfolder'], args=args)



if __name__ == "__main__":
    main()